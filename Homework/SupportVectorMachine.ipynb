{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support Vector Machine Homework\n",
        "\n",
        "In this homework you'll write your own code to train a support vector machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with the two-point synthetic dataset from class. This is just to check that our code is working properly, using an example where we know the true solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "#Parameters\n",
        "lamb = 0.1\n",
        "beta = 1\n",
        "alpha = 1\n",
        "\n",
        "#label at z is +1 and -z is -1\n",
        "#Optimal w = z / ||z||^2 and b=0\n",
        "z = np.array([1,2])\n",
        "\n",
        "#Data matrix and labels\n",
        "X = np.vstack((z,-z))\n",
        "y = np.array([1,-1])\n",
        "\n",
        "#Random initializztion\n",
        "w = np.random.randn(2)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "    #Insert your code here to compute the gradients and loss. You can\n",
        "    #Use as many additional lines of code as needed (i.e., don't try too\n",
        "    #hard to put the whole computation in one line)\n",
        "    grad_w =\n",
        "    grad_b =\n",
        "    loss =\n",
        "\n",
        "    w -= alpha*grad_w\n",
        "    b -= alpha*grad_b\n",
        "    if i % 100 == 0:\n",
        "        print('Iteration',i,'Loss =',loss,'w =',w,'b =',b,'Prediction =',np.sign(X@w-b))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's move on to apply our algorithm to real data from the MNIST dataset. We now install [GraphLearning](https://github.com/jwcalder/GraphLearning) and load the MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install graphlearning -q"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "\n",
        "data,labels = gl.datasets.load('mnist')\n",
        "gl.utils.image_grid(data,n_rows=25,n_cols=25)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now create a binary classification problem to classify pairs of MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "digits = (4,9)\n",
        "\n",
        "#Subset to pair of digits\n",
        "mask = (labels == digits[0]) | (labels == digits[1]) #Logical or\n",
        "X = data[mask,:]\n",
        "y = labels[mask].astype(float)\n",
        "\n",
        "#convert to -1,1 labels\n",
        "y = y -  np.min(y) - 1\n",
        "y[y>-1] = 1\n",
        "\n",
        "#We now standardize the data to range of -1 to 1\n",
        "X -= X.min()\n",
        "X /= X.max()\n",
        "X = 2*X-1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now perform a train/test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we train the svm on the training set, evaluating testing accuracy at each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Size of data\n",
        "m,n = X_train.shape\n",
        "\n",
        "#Parameters\n",
        "lamb = 0.01\n",
        "beta = 1\n",
        "alpha = 0.1\n",
        "\n",
        "#Random initialization\n",
        "w = np.random.randn(n)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "    #Insert your code here to compute the gradients and loss. You can\n",
        "    #Use as many additional lines of code as needed (i.e., don't try too\n",
        "    #hard to put the whole computation in one line)\n",
        "    grad_w =\n",
        "    grad_b =\n",
        "    loss =\n",
        "\n",
        "    w -= alpha*grad_w\n",
        "    b -= alpha*grad_b\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        train_acc = round(100*np.mean(np.sign(X_train@w - b) == y_train),2)\n",
        "        test_acc = round(100*np.mean(np.sign(X_test@w - b) == y_test),2)\n",
        "        print('Iteration',i,'Loss =',loss,'Train Accuracy =',train_acc,'Test Accuracy =',test_acc)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
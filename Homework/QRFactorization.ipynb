{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QR Factorization\n",
        "\n",
        "The QR factorization of a square matrix $A$ produces an orthogonal matrix $Q$ and an upper triangular matrix $R$ such that\n",
        "$$A = QR.$$\n",
        "The QR factorization is useful for solving the linear equation $A\\mathbf{x}=\\mathbf{b}$ by solving the equivalent equation $QR\\mathbf{x}=\\mathbf{b}$, or rather\n",
        "$$R\\mathbf{x} = Q^T \\mathbf{b},$$\n",
        "where we recall that $Q^T$ is the inverse of $Q$, since $Q$ is orthonormal. As $R$ is upper triangular, we can solve $R\\mathbf{x}=Q^T\\mathbf{b}$ by back substitution. While we focus on square matrices in this notebook, it is important to note that QR factorization can also be applied to non-square matrices, in which case there are interesting connections to least squares solutions to linear systems.\n",
        "\n",
        "## QR via Gram-Schmidt\n",
        "\n",
        "The simplest way to compute a QR factorization is with the Gram-Schmidt algorithm. We assume $A$ is a square $n\\times n$ non-singular matrix (i.e., its columns are linearly independent). The main steps of the Gram-Schmidt algorithm are presented in pseudocode below.\n",
        "\n",
        "Let $\\mathbf{a}_1,\\mathbf{a}_2,\\dots,\\mathbf{a}_n$ denote the columns of $A$. We initialize $\\mathbf{q}_1=\\mathbf{a}_1/r_{11}$, where $r_{11}=\\|\\mathbf{a}_1\\|$ and repeat the steps below for $k=2$ through $k=n$.\n",
        "\n",
        "1. Compute $r_{jk} = \\mathbf{q}_j\\cdot \\mathbf{a}_k$ for $j \\leq k-1$ and $r_{jk}=0$ for $j>k$.\n",
        "2. Compute $\\mathbf{x}_k = \\mathbf{a}_k - \\sum_{j=1}^{k-1} r_{jk}\\mathbf{q}_j$ and $r_{kk} = \\|\\mathbf{x}_k\\|$.\n",
        "3. Compute the $k$th column of $Q$, given by $\\mathbf{q}_k = \\frac{\\mathbf{x}_k}{r_{kk}}$.\n",
        "\n",
        "The vectors $\\mathbf{q}_k$ form the columns of $Q$, and by definition\n",
        "$$\\mathbf{a}_k = \\sum_{j=1}^k r_{jk} \\mathbf{q}_j,$$\n",
        "for all $k$, which is equivalent to the QR factorization statement\n",
        "$$A = QR.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise\n",
        "\n",
        "Write Python code for QR factorization using Gram-Schmidt below. You can use the template in the code below. Try your algorithm on some toy matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def QR_GS(A):\n",
        "    \"\"\"QR via Gram-Schmidt\n",
        "    ======\n",
        "    Produces a QR factorization via standard Gram-Schmidt.\n",
        "    The algorithm is numerically unstable and may not return orthonormal Q.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A : numpy array, float\n",
        "        Non-singular matrix to perform QR on. A should be mxn with m >= n\n",
        "        and all columns linearly independent.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Q : numpy array, float\n",
        "        Orthogonal basis.\n",
        "    R : numpy array, float\n",
        "        Upper triangular matrix R so that A=QR\n",
        "    \"\"\"\n",
        "\n",
        "    #Get shapes of matrices and initialize Q,R\n",
        "    m,n = A.shape\n",
        "    Q = np.zeros((m,n))\n",
        "    R = np.zeros((n,n))\n",
        "\n",
        "    #First step \n",
        "    R[0,0] = \n",
        "    Q[:,0] = \n",
        "\n",
        "    #The code you insert below can be vectorized\n",
        "    #so that only one line is required for each computation. \n",
        "    #Alternatively you can add loops.\n",
        "    for k in range(1,m):\n",
        "        R[:k,k] =  #Define entries of R\n",
        "        xk = \n",
        "        R[k,k] = \n",
        "        Q[:,k] = \n",
        "\n",
        "    return Q,R"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example, we define the matrix\n",
        "$$A = \\begin{pmatrix}\n",
        "1 & 1\\\\\n",
        "0 & 1\n",
        "\\end{pmatrix}.$$\n",
        "The QR factorization is $A = I A$, where $I$ is the identity matrix (in fact, we always have $Q=I$ for any $A$ that is already upper triangular). We can verify this in Python code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A = np.array([[1,0],[1,1]])\n",
        "print(\"A=\\n\",A.T)\n",
        "\n",
        "Q,R = QR_GS(A.T)\n",
        "print(\"Q=\\n\",Q)\n",
        "print(\"R=\\n\",R)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code above, replace $A$ with $A^T$. How does the QR factorization change? Verify the QR factorization for $A^T$ by hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us check below that the QR factorization works for randomly generated matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A = np.random.rand(10,10)\n",
        "Q,R = QR_GS(A)\n",
        "print(np.linalg.norm(A - Q@R))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see above that the norm $\\|A-QR\\|$ is close to machine precision, around $10^{-15}$ in this case. We can also check the orthogonality of $Q$ with the code below, which computes the norm $\\|I - Q^TQ\\|$. Clearly $Q$ is very close to orthogonal, up to machine precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(np.linalg.norm(Q.T@Q - np.eye(Q.shape[0])))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss of orthogonality and numerical instabilities\n",
        "\n",
        "In exact arithmetic, the Gram-Schmidt procedure works to produce a valid QR factorization. However, in the inexact world of floating point arithmetic, the method is numerically unstable and floating point roundoff errors accumulate, leading to a loss of orthogonality in $Q$. When $Q$ is not orthogonal, the definition of $R$ is incorrect and does not lead to a correct factorization.\n",
        "\n",
        "The instabilities in Gram-Schmidt can be observed for very large matrices, or for ill-conditioned matrices. Our first example is for large matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A = np.random.rand(1000,1000)\n",
        "Q,R = QR_GS(A)\n",
        "print(np.linalg.norm(A - Q@R))\n",
        "print(np.linalg.norm(Q.T@Q - np.eye(Q.shape[0])))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that both $\\|A-QR\\|$ and $\\|I - Q^TQ\\|$ are much larger than the machine precision of around $10^{-15}$ we saw earlier, but they are still quite small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ill-conditioned matrices\n",
        "\n",
        "Random matrices are in fact quite easy to compute with as they are often well-conditioned. The situation can be substantially worse for other matrices that are poorly conditioned. An example of an ill-conditioned matrix is the Hilbert matrix\n",
        "$$A = \\begin{pmatrix}\n",
        "1 & \\frac12 & \\frac13 & \\cdots & \\frac1n\\\\\n",
        "\\frac12 & \\frac13 & \\frac14 & \\cdots & \\frac{1}{n+1}\\\\\n",
        "\\frac13& \\frac14 & \\frac15 & \\cdots & \\frac{1}{n+2}\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac1n & \\frac{1}{n+1} & \\frac{1}{n+2} & \\cdots & \\frac{1}{2n-1}\n",
        "\\end{pmatrix}.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def hilbert_matrix(n):\n",
        "    \"\"\"Hilbert Matrix\n",
        "    ======\n",
        "    Returns the nxn Hilbert matrix with (i,j) entry 1/(i+j+1), i,j=0,...,n-1\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n : int\n",
        "        Size of matrix (nxn)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A : numpy array, float\n",
        "        Hilbert Matrix\n",
        "    \"\"\"\n",
        "\n",
        "    x = np.arange(n)[:,None]\n",
        "    A = 1/(x + x.T + 1)\n",
        "    return A"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the performance of your QR algorithm on the Hilbert matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('Original Gram-Schmidt')\n",
        "A = hilbert_matrix(20)\n",
        "Q,R = QR_GS(A)\n",
        "print('||A-QR||=',np.linalg.norm(A - Q@R))\n",
        "print('||I - Q^TQ||',np.linalg.norm(Q.T@Q - np.eye(Q.shape[0])))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your code should produce $Q$ and $R$ with $A=QR$ up to machine precision, but the the matrix should $Q$ fail to be orthogonal. The loss of orthogonlity arises from floating point round-off errors that accumulate. These arise from the step $\\mathbf{x}_k = \\mathbf{a}_k - \\sum_{j=1}^{k-1} r_{jk}\\mathbf{q}_j$ when the result $\\mathbf{x}_k$ is very small compared to the two terms in the difference (i.e., $\\mathbf{a}_k$ is nearly in the span $\\mathbf{q}_1,\\dots,\\mathbf{q_{k-1}}$). This occurs when the matrix $A$ is ill-conditioned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Floating point round-off errors\n",
        "\n",
        "If this is your first experience thinking about floating point numbers and roundoff errors, try running the code below, which should return 1 (it does not, due to accumulation of floating point roundoff errors, which is particularly bad when operating with very small and very large numbers at the same time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = int(1e6)\n",
        "b = 1e6\n",
        "c = b\n",
        "\n",
        "#In exact arithmetic, the loop below just adds 1 to c, and is the same as c=c+1\n",
        "#In floating point arithmetic, the errors in c += 1/n accumulate, expecially\n",
        "#since 1/n is far smaller than c.\n",
        "for i in range(n):\n",
        "    c += 1/n\n",
        "\n",
        "#Since we start at c=b and the loop above should just perform c=c+1\n",
        "#we should have c=b+1 and so c-b=1. This is not the case in floating point\n",
        "#arithmetic.\n",
        "print(c-b)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below are some even simpler examples of round-off error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(1+1e-16)\n",
        "print(0.6 == 0.6)\n",
        "print(0.1 + 0.2 + 0.3 == 0.6)\n",
        "print(0.1 + 0.2 + 0.3)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Re-orthogonalization\n",
        "\n",
        "There are several ways to address the numerical instability of Gram--Schmidt for QR factorization. Here, we will use the re-orthogonalization trick, which essentially just repeats the orthogonalization step a second time. The steps are given below.\n",
        "\n",
        "1. Compute $s_{jk} = \\mathbf{q}_j\\cdot \\mathbf{a}_k$ for $j \\leq k-1$.\n",
        "2. Compute $\\mathbf{v} = \\mathbf{a}_k - \\sum_{j=1}^{k-1} s_{jk}\\mathbf{q}_j$.\n",
        "3. Compute $t_{jk} = \\mathbf{q}_j\\cdot \\mathbf{v}$ for $j \\leq k-1$.\n",
        "4. Compute $\\mathbf{x}_k = \\mathbf{v} - \\sum_{j=1}^{k-1} t_{jk}\\mathbf{q}_j$.\n",
        "5. Set $r_{jk} = s_{jk} + t_{jk}$ for $j \\leq k-1$.\n",
        "3. Set $r_{kk} = \\|\\mathbf{x}_k\\|$ and $\\mathbf{q}_k = \\frac{\\mathbf{x}_k}{r_{kk}}$.\n",
        "\n",
        "Steps 1-2 are the first orthogonalization, while steps 3-4 are the second one. In exact arithmetic we have $t_{jk}=0$ and steps 3-4 do nothing. In in-exact floating point arithmetic, steps 3-4 correct for a loss of orthogonality in the computation of $\\mathbf{v}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise\n",
        "\n",
        "Implement the Gram--Shmidt with re-orthogonalization in Python. Use the code template below. You may want to first *vectorize* your original Gram-Schmidt code so it does not have any loops, aside from the outer loop over $k$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def QR_GS_RO(A):\n",
        "    \"\"\"QR via Gram-Schmidt with Re-orthogonalization\n",
        "    ======\n",
        "    Produces a QR factorization via standard Gram-Schmidt.\n",
        "    The algorithm is numerically unstable and may not return orthonormal Q.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A : numpy array, float\n",
        "        Non-singular matrix to perform QR on. A should be mxn with m >= n\n",
        "        and all columns linearly independent.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Q : numpy array, float\n",
        "        Orthogonal basis.\n",
        "    R : numpy array, float\n",
        "        Upper triangular matrix R so that A=QR\n",
        "    \"\"\"\n",
        "\n",
        "    #Get shapes of matrices and initialize Q,R\n",
        "    m,n = A.shape\n",
        "    Q = np.zeros((m,n))\n",
        "    R = np.zeros((n,n))\n",
        "\n",
        "    #First step \n",
        "    R[0,0] = \n",
        "    Q[:,0] = \n",
        "\n",
        "    for k in range(1,m):\n",
        "\n",
        "        #First orthogonalization\n",
        "        s = \n",
        "        v = \n",
        "\n",
        "        #Re-orthogonalization\n",
        "        t = \n",
        "        xk = \n",
        "\n",
        "        #Set entries of Q and R\n",
        "        R[:k,k] = s + t\n",
        "        R[k,k] = np.linalg.norm(xk)\n",
        "        Q[:,k] = xk/R[k,k]\n",
        "\n",
        "    return Q,R"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try the method on the Hilbert matrix. If your code is correct, you should find that $Q$ is orthogonal up to machine precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('\\nGram-Schmidt with reorthogonalization')\n",
        "A = hilbert_matrix(20)\n",
        "Q,R = QR_GS_RO(A)\n",
        "print('||A-QR||=',np.linalg.norm(A - Q@R))\n",
        "print('||I - Q^TQ||',np.linalg.norm(Q.T@Q - np.eye(Q.shape[0])))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. QR algorithms in Python packages.\n",
        "  * Find an implementation of QR factorization in Numpy or Scipy (or any other Python package).\n",
        "  * Compare the implementation of QR that you found to the original Gram--Schmidt as well as the re-orthogonalized version. Compute both $\\|A-QR\\|$ and $\\|I - Q^TQ\\|$ for all three methods. Try random matrices, and ill-conditioned ones.\n",
        "  * Can you find out what algorithm is used to compute QR for the implementation you found?\n",
        "  * Compare the run-times of all three algorithms for large matrices, say around $1000\\times 1000$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "A = hilbert_matrix(20)\n",
        "\n",
        "#Gram-Schmidt QR\n",
        "print('\\nGram-Schmidt')\n",
        "Q,R = QR_GS(A)\n",
        "print(np.linalg.norm(A - Q@R,ord=np.inf))\n",
        "print(np.linalg.norm(Q.T@Q - np.eye(Q.shape[0])))\n",
        "\n",
        "#Gram-Schmidt QR with re-orthogonalization\n",
        "print('\\nGram-Schmidt with re-orthogonalization')\n",
        "Q,R = QR_GS_RO(A)\n",
        "print(np.linalg.norm(A - Q@R,ord=np.inf))\n",
        "print(np.linalg.norm(Q.T@Q - np.eye(Q.shape[0])))\n",
        "\n",
        "#Insert your code to compare to Numpy or Scipy\n",
        "print('\\nNumpy or Scipy')\n",
        "\n",
        "#Computation time\n",
        "A = np.random.rand(2000,2000)\n",
        "start_time = time.time()\n",
        "Q,R = QR_GS(A)\n",
        "print(\"\\nGram-Schmidt: %s seconds.\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "Q,R = QR_GS_RO(A)\n",
        "print(\"\\nGram-Schmidt with re-orthogonalization: %s seconds.\" % (time.time() - start_time))\n",
        "\n",
        "#Insert your code to compare to Numpy or Scipy\n",
        "print(\"\\nNumpy or Scipy: %s seconds.\" % (time.time() - start_time))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Write Python code to solve the linear system $A\\mathbf{x}=\\mathbf{b}$ with $QR$ factorization. Test your method on some large random matrices. You can use `np.linalg.inv` to compute the inverse of the upper triangular matrix $R$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 100\n",
        "A = np.random.rand(n,n)\n",
        "x_true = np.random.rand(n,1)\n",
        "b = A@x_true\n",
        "\n",
        "#Insert your code to solve the linear system Ax=b with your QR code\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
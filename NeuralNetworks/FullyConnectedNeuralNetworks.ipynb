{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fully Connected Neural Networks\n",
        "\n",
        "This notebook gives an introduction to training fully connected neural networks in PyTorch. Turn on the GPU in Edit -> Notebook Settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function approximation\n",
        "\n",
        "We first consider approximating a one dimensional function with a neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#The function to learn\n",
        "a = [5,4,3,2]\n",
        "k = [3,2*3.14159,10*2.718,15*3.14159]\n",
        "def myfunc(x):\n",
        "    y = a[0]*torch.sin(k[0]*x)\n",
        "    for i in range(1,len(a)):\n",
        "        y += a[i]*torch.sin(k[i]*x)\n",
        "        y += a[i]*torch.cos(k[i]*x)\n",
        "    return y\n",
        "\n",
        "#Training data on [0,1] and apply myfunc\n",
        "data = torch.arange(0,1,0.001).unsqueeze(1)  #Unsqueeze makes it 100x1 instead of 1D length 100\n",
        "target = myfunc(data)\n",
        "\n",
        "#Plot the function\n",
        "fig = plt.figure()\n",
        "plt.plot(data,target,label='Target Function') "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now construct our one hidden layer neural network in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_hidden=1000):\n",
        "        super(Net, self).__init__()\n",
        "        self.n = num_hidden\n",
        "        self.fc1 = nn.Linear(1,num_hidden)\n",
        "        self.fc2 = nn.Linear(num_hidden,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        return self.fc2(x)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below trains the network and plots the result. The flag `cuda` controls whether to use the GPU. Notice the data and model must be sent to the GPU, and pulled back to the cpu for plotting and printing. To use the GPU in Colab, go to Edit -> Notebook Settings, and enable the GPU (you'll have to restart the notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#GPU\n",
        "cuda = True\n",
        "use_cuda = cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using ',device)\n",
        "\n",
        "T = 30000 #Number of training iterstions\n",
        "model = Net(num_hidden=10**3).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  #Learning rate\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001)  #Regular gradient descent (very slow)\n",
        "data,target = data.to(device), target.to(device)\n",
        "\n",
        "#Training \n",
        "model.train()  #Put into training mode\n",
        "for i in range(T):\n",
        "\n",
        "    #Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #Pass data through model and loss (gradients get accumulated in the optimizer)\n",
        "    output = model(data)\n",
        "    loss = torch.mean((output-target)**2)\n",
        "\n",
        "    #Back propagation to compute all gradients, and an optimizer step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #Print Loss only every 1000 iterations\n",
        "    if i % 1000 == 0:\n",
        "        print('Iteration: %d, Loss: %f'%(i,loss.item()))\n",
        "\n",
        "#Plot the function and neural network\n",
        "model.eval()\n",
        "fig = plt.figure()\n",
        "plt.plot(data.cpu(),target.cpu(),label='Target Function') \n",
        "plt.plot(data.cpu(),output.detach().cpu(),label='Neural Network') \n",
        "plt.legend()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Toy classification problem\n",
        "\n",
        "We now consider a simple toy classification problems in two dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ClassifyNet(nn.Module):\n",
        "    def __init__(self, num_in, num_hidden, num_out):\n",
        "        super(ClassifyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_in,num_hidden)\n",
        "        self.fc2 = nn.Linear(num_hidden,num_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.fc2(F.relu(self.fc1(x))), dim=1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now train the model for classification and plot the resulting decision boundary. Here, we do not use the GPU since the data sets are small and training is very quick. We first define our usual plot_region function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_region(X,L,clf,alpha=0.5,cmap='Paired',cp=np.array([0.5,2.5,6.5,8.5,10.5,11.5]),markers = ['o','s','D','^','v','p'],vmin=0,vmax=12,markersize=75,linewidths=1.25,markerlinewidths=1,res=0.01,train_pts=None):\n",
        "\n",
        "    plt.figure()\n",
        "    x,y = X[:,0],X[:,1]\n",
        "    xmin, xmax = np.min(x),np.max(x)\n",
        "    ymin, ymax = np.min(y),np.max(y)\n",
        "    f =0.1*np.maximum(np.max(np.abs(x)),np.max(np.abs(y)))\n",
        "    xmin -= f\n",
        "    ymin -= f\n",
        "    xmax += f\n",
        "    ymax += f\n",
        "    c = cp[L]\n",
        "    c_u = np.unique(c)\n",
        "    \n",
        "    for i,color in enumerate(c_u):\n",
        "        sub = c==color\n",
        "        plt.scatter(x[sub],y[sub],zorder=2,c=c[sub],cmap=cmap,edgecolors='black',vmin=vmin,vmax=vmax,linewidths=markerlinewidths,marker=markers[i],s=markersize)\n",
        "        if train_pts is not None:\n",
        "            plt.scatter(x[sub & train_pts],y[sub & train_pts],zorder=2,c=np.ones(np.sum(sub&train_pts))*5.5,cmap=cmap,edgecolors='black',vmin=vmin,vmax=vmax,linewidths=markerlinewidths,marker=markers[i],s=markersize)\n",
        "\n",
        "\n",
        "    X,Y = np.mgrid[xmin:xmax:0.01,ymin:ymax:0.01]\n",
        "    points = np.c_[X.ravel(),Y.ravel()]\n",
        "    z = clf.predict(points)\n",
        "    z = z.reshape(X.shape)\n",
        "    plt.contourf(X, Y, cp[z],alpha=alpha,cmap=cmap,antialiased=True,vmin=vmin,vmax=vmax)\n",
        "\n",
        "    X,Y = np.mgrid[xmin:xmax:res,ymin:ymax:res]\n",
        "    points = np.c_[X.ravel(),Y.ravel()]\n",
        "    if len(np.unique(c)) == 2:\n",
        "\n",
        "        if hasattr(clf, \"decision_function\"):\n",
        "            z = clf.decision_function(points)\n",
        "        else:\n",
        "            z = clf.predict_proba(points)\n",
        "            z = z[:,0] - z[:,1] + 1e-15\n",
        "        z = z.reshape(X.shape)\n",
        "        plt.contour(X, Y, z, [0], colors='black',linewidths=linewidths,antialiased=True)\n",
        "    else:\n",
        "        z = clf.predict(points)\n",
        "        z = z.reshape(X.shape)\n",
        "        plt.contour(X, Y, z, colors='black',linewidths=linewidths,antialiased=True)\n",
        "    plt.xlim((xmin,xmax))\n",
        "    plt.ylim((ymin,ymax))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.optim as optim\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Create a Python class so it looks like an sklearn classifier\n",
        "class NetWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def predict_proba(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = torch.from_numpy(x).float()\n",
        "            p = self.model(x).numpy()\n",
        "        return p\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.argmax(self.predict_proba(x),axis=1)\n",
        "\n",
        "#Data set\n",
        "n = 100\n",
        "#X,L = datasets.make_blobs(n_samples=n, cluster_std=[1,1.5], centers=2, random_state=1)\n",
        "X,L = datasets.make_moons(n_samples=n,noise=0.1,random_state=4)\n",
        "#X,L = datasets.make_circles(n_samples=n,noise=0.1,random_state=4,factor=0.5)\n",
        "\n",
        "#Setup model and optimizer\n",
        "num_hidden = 64 #Number of hidden nodes in the one hidden layer\n",
        "model = ClassifyNet(2,num_hidden,2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  #Learning rates\n",
        "\n",
        "#Data to torch\n",
        "data = torch.from_numpy(X).float()\n",
        "target = torch.from_numpy(L).long()\n",
        "\n",
        "#Train for 1000 epochs\n",
        "model.train()\n",
        "for i in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    loss = F.nll_loss(model(data), target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print('Final Loss=%f'%loss.item())\n",
        "model.eval()\n",
        "plot_region(X,L,NetWrapper(model))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Application to MNIST digit classification\n",
        "\n",
        "Our last example is to the classification of MNIST digits. While Torch offers access to MNIST and other datasets, we'll use GraphLearning for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install -q graphlearning"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first load MNIST and display some images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "\n",
        "#Load MNIST data\n",
        "x,y = gl.datasets.load('mnist')\n",
        "\n",
        "#Display images\n",
        "gl.utils.image_grid(x,n_rows=16,n_cols=16)\n",
        "print(x.shape)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a two layer neural network with 64 hidden nodes, and the Adam Optimizer and SGD with batch size 480. You can play around with these parameters and see how the training is affected. We also use the full 60000 training set, but you can experiment with using less training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_hidden = 64\n",
        "batch_size = 480\n",
        "\n",
        "#GPU\n",
        "cuda = True\n",
        "use_cuda = cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "#Training data (select at random from first 600000)\n",
        "train_size = 60000\n",
        "train_ind = np.random.permutation(60000)[:train_size]\n",
        "\n",
        "#Convert data to torch and device\n",
        "data_train = torch.from_numpy(x[train_ind,:]).float().to(device)\n",
        "target_train = torch.from_numpy(y[train_ind]).long().to(device)\n",
        "data_test = torch.from_numpy(x[60000:,:]).float().to(device)\n",
        "target_test = torch.from_numpy(y[60000:]).long().to(device)\n",
        "\n",
        "#Setup model and optimizer\n",
        "model = ClassifyNet(784,num_hidden,10).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  #Learning rates\n",
        "\n",
        "#Training \n",
        "print('Iteration,Testing Accuracy,Training Accuracy')\n",
        "for i in range(20):\n",
        "\n",
        "    #Model evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = torch.argmax(model(data_test),axis=1)\n",
        "        test_accuracy = torch.sum(pred == target_test)/len(pred)\n",
        "        pred = torch.argmax(model(data_train),axis=1)\n",
        "        train_accuracy = torch.sum(pred == target_train)/len(pred)\n",
        "        print(i,test_accuracy.item()*100,train_accuracy.item()*100)\n",
        "\n",
        "    #Training mode, run data through neural network in mini-batches (SGD)\n",
        "    for j in range(0,len(target_train),batch_size):\n",
        "        model.train()  \n",
        "        optimizer.zero_grad()\n",
        "        loss = F.nll_loss(model(data_train[j:j+batch_size,:]), target_train[j:j+batch_size])\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Exercises\n",
        "1. Try reducing the train size and see if you can get the network to overfit (which means the training accuracy is much larger than the testing accuracy).\n",
        "2. Try changing the number of hidden nodes, and the number of layers in the network. How is the accuracy affected?\n",
        "3. Pick a new classification dataset publicly available online. For example, you can browse [Kaggle](https://www.kaggle.com/) for general data science datasets, [Torch Datasets](https://pytorch.org/vision/stable/datasets.html) for image classification problems, [sklearn datasets](https://scikit-learn.org/stable/datasets.html), or [GraphLearning](https://jwcalder.github.io/GraphLearning/datasets.html#graphlearning.datasets.load). Train a neural network classifier on your new dataset. The code below will get you started with graphlearning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "\n",
        "data,labels = gl.datasets.load('signmnist')\n",
        "gl.utils.image_grid(data)\n",
        "\n",
        "data,labels = gl.datasets.load('fashionmnist')\n",
        "gl.utils.image_grid(data)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
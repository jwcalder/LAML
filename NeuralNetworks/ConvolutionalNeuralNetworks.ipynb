{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "This notebook gives an introduction to training convolutional neural networks in PyTorch to classify MNIST digits. Turn on the GPU in Edit -> Notebook Settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we construct a small convolutional neural network with 2 convolutional layers with 32 and 64 channels. Between each layer is a 2x2 max pool. After the two convolutions and max poolings, the 28x28 MNIST digits are mapped to 64 5x5 images. The 64*5*5=1600 flattened features are then fed into a one hidden layer fully connected neural network with 1024 hidden neurons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self,w=(32,64)):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, w[0], 3, 1)\n",
        "        self.conv2 = nn.Conv2d(w[0], w[1], 3, 1)\n",
        "        self.fc1 = nn.Linear(w[1]*5*5, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # images start out 28x28\n",
        "        x = self.conv1(x)  #26x26 images\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)  #13x13 images\n",
        "        x = self.conv2(x)  #11x11 images\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)  #5x5 images\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we define functions to implement 1 training epoch using stochastic gradient descent and compute the test accuracy. These functions make use of PyTorch data loaders, which are convenient ways to load data sets and access minibatches for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below trains the convolutional neural network and plots the result. The flag `cuda` controls whether to use the GPU. Notice the data and model must be sent to the GPU, and pulled back to the cpu for plotting and printing. To use the GPU in Colab, go to Edit -> Notebook Settings, and enable the GPU (you'll have to restart the notebook).\n",
        "\n",
        "Note below we use the Adadelta optimizer instead of the Adam optimizer, which gives better results in this setting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#Training settings\n",
        "cuda = True   #Use GPU acceleration (Edit->Notebook Settings and enable GPU)\n",
        "batch_size = 64\n",
        "test_batch_size = 1000\n",
        "learning_rate = 1.0  \n",
        "epochs = 20\n",
        "\n",
        "#GPU\n",
        "use_cuda = cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "#Train and Test Data Loader Setup\n",
        "train_kwargs = {'batch_size': batch_size}\n",
        "test_kwargs = {'batch_size': test_batch_size}\n",
        "\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,'pin_memory': True,'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "dataset1 = datasets.MNIST('./data', train=True, download=True,transform=transform)\n",
        "dataset2 = datasets.MNIST('./data', train=False, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "#Set up model, optimizer, and scheduler\n",
        "model = CNN().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Exercises\n",
        "1. Try using the Adam optimizer. Can you get similar results?\n",
        "2. Try reducing the train size and see if you can get the network to overfit (which means the training accuracy is much larger than the testing accuracy).\n",
        "3. Try another dataset from Torch, like FashionMNIST or Cifar10. You'll have to change the network for Cifar-10, since the images are 32x32x3 color images."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
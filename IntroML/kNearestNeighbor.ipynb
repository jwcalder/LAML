{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# k-nearest neighbor classification\n",
        "\n",
        "This notebook is a brief introduction to support vector machines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first train k-nearest neighbor classifiers on some toy blob datasets. For this, we will define a function that helps plot the classification decision regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_region(X,L,clf,alpha=0.5,cmap='Paired',cp=np.array([0.5,2.5,6.5,8.5,10.5,11.5]),\n",
        "                markers = ['o','s','D','^','v','p'],vmin=0,vmax=12,fname=None,markersize=75,\n",
        "                linewidths=1.25,markerlinewidths=1,res=0.1,train_pts=None):\n",
        "\n",
        "    plt.figure()\n",
        "    x,y = X[:,0],X[:,1]\n",
        "    xmin, xmax = np.min(x),np.max(x)\n",
        "    ymin, ymax = np.min(y),np.max(y)\n",
        "    f =0.1*np.maximum(np.max(np.abs(x)),np.max(np.abs(y)))\n",
        "    xmin -= f\n",
        "    ymin -= f\n",
        "    xmax += f\n",
        "    ymax += f\n",
        "    c = cp[L]\n",
        "    c_u = np.unique(c)\n",
        "\n",
        "    for i,color in enumerate(c_u):\n",
        "        sub = c==color\n",
        "        plt.scatter(x[sub],y[sub],zorder=2,c=c[sub],cmap=cmap,edgecolors='black',vmin=vmin,vmax=vmax,linewidths=markerlinewidths,marker=markers[i],s=markersize)\n",
        "        if train_pts is not None:\n",
        "            plt.scatter(x[sub & train_pts],y[sub & train_pts],zorder=2,c=np.ones(np.sum(sub&train_pts))*5.5,cmap=cmap,edgecolors='black',vmin=vmin,vmax=vmax,linewidths=markerlinewidths,marker=markers[i],s=markersize)\n",
        "\n",
        "\n",
        "    X,Y = np.mgrid[xmin:xmax:res,ymin:ymax:res]\n",
        "    points = np.c_[X.ravel(),Y.ravel()]\n",
        "    z = clf.predict(points)\n",
        "    z = z.reshape(X.shape)\n",
        "    plt.contourf(X, Y, cp[z],alpha=alpha,cmap=cmap,antialiased=True,vmin=vmin,vmax=vmax)\n",
        "\n",
        "    X,Y = np.mgrid[xmin:xmax:res,ymin:ymax:res]\n",
        "    points = np.c_[X.ravel(),Y.ravel()]\n",
        "    if len(np.unique(c)) == 2:\n",
        "\n",
        "        if hasattr(clf, \"decision_function\"):\n",
        "            z = clf.decision_function(points)\n",
        "        else:\n",
        "            z = clf.predict_proba(points)\n",
        "            z = z[:,0] - z[:,1] + 1e-15\n",
        "        z = z.reshape(X.shape)\n",
        "        plt.contour(X, Y, z, [0], colors='black',linewidths=linewidths,antialiased=True)\n",
        "    else:\n",
        "        z = clf.predict(points)\n",
        "        z = z.reshape(X.shape)\n",
        "        plt.contour(X, Y, z, colors='black',linewidths=linewidths,antialiased=True)\n",
        "    plt.xlim((xmin,xmax))\n",
        "    plt.ylim((ymin,ymax))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now generate some blob data, train an k-nearest neighbor classifier using sklearn, and plot the resulting decision regions. Try varying the number of centers (classes), numbers of data points, number of neighbors, and the metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import datasets\n",
        "\n",
        "n = 100\n",
        "X,L = datasets.make_blobs(n_samples=n, centers=3)\n",
        "clf = KNeighborsClassifier(n_neighbors=10, metric='euclidean') #Try 'cosine' or 'cityblock'\n",
        "clf.fit(X,L)\n",
        "\n",
        "#Plot the region\n",
        "plot_region(X,L,clf,res=0.1) #decrease res to get higher resolution of boundary, but can be very slow"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now consdier some real data. We will use the breast cancer classification dataset from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = datasets.load_breast_cancer()\n",
        "print(data.DESCR)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below performs a random train/test split, trains a knn classifier on the training set, and evaluates the performance. We also compare against SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = data.data\n",
        "y = data.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\n",
        "\n",
        "#Train k-nn classifier\n",
        "clf = KNeighborsClassifier(n_neighbors=10)\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "#Training accuracy\n",
        "y_pred = clf.predict(x_train)\n",
        "train_acc = np.mean(y_pred == y_train)*100\n",
        "print('knn Training Accuracy: %.2f%%'%train_acc)\n",
        "\n",
        "#Testing accuracy\n",
        "y_pred = clf.predict(x_test)\n",
        "test_acc = np.mean(y_pred == y_test)*100\n",
        "print('knn Testing Accuracy: %.2f%%'%test_acc)\n",
        "\n",
        "#Train SVM\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "#Training accuracy\n",
        "y_pred = clf.predict(x_train)\n",
        "train_acc = np.mean(y_pred == y_train)*100\n",
        "print('SVM Training Accuracy: %.2f%%'%train_acc)\n",
        "\n",
        "#Testing accuracy\n",
        "y_pred = clf.predict(x_test)\n",
        "test_acc = np.mean(y_pred == y_test)*100\n",
        "print('SVM Testing Accuracy: %.2f%%'%test_acc)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercises\n",
        "\n",
        "1. Play round with different metrics and number of neighbors for the k-nearest neighbor classifier.\n",
        "2. Try running the classificadtion over many different random train/test splits and report the average and standard deviation of accuracy. (You can also use a k-fold cross-validation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handwritten digit recognition\n",
        "\n",
        "Here we tackle the problem of handwritten digit recognition with several basic machine learning algorithms. We will first install the [GraphLearning](https://github.com/jwcalder/GraphLearning) Python package, which gives easy access to the MNIST digits dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install graphlearning -q"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now load the MNIST dataset and display some of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "\n",
        "digits,labels = gl.datasets.load('mnist')\n",
        "print(digits.shape)\n",
        "print(labels.shape)\n",
        "gl.utils.image_grid(digits)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The standard train/test split for MNIST is to take the first 60000 images as training, and the last 10000 as testing. Let's run a k-nearest neighbor classifier and a support vector machine (VM) on this train/test split. The code will take a long time to run (5-10 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#standard train/test split\n",
        "x_train = digits[:60000,:]\n",
        "x_test = digits[60000:,:]\n",
        "y_train = labels[:60000]\n",
        "y_test = labels[60000:]\n",
        "\n",
        "#Train SVM\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "#Training accuracy\n",
        "y_pred = clf.predict(x_train)\n",
        "train_acc = np.mean(y_pred == y_train)*100\n",
        "print('SVM Training Accuracy: %.2f%%'%train_acc)\n",
        "\n",
        "#Testing accuracy\n",
        "y_pred = clf.predict(x_test)\n",
        "test_acc = np.mean(y_pred == y_test)*100\n",
        "print('SVM Testing Accuracy: %.2f%%'%test_acc)\n",
        "\n",
        "#Train k-nn classifier\n",
        "clf = KNeighborsClassifier(n_neighbors=10, metric='cosine')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "#Training accuracy\n",
        "y_pred = clf.predict(x_train)\n",
        "train_acc = np.mean(y_pred == y_train)*100\n",
        "print('knn Training Accuracy: %.2f%%'%train_acc)\n",
        "\n",
        "#Testing accuracy\n",
        "y_pred = clf.predict(x_test)\n",
        "test_acc = np.mean(y_pred == y_test)*100\n",
        "print('knn Testing Accuracy: %.2f%%'%test_acc)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot some of the missclassified images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_show = 20\n",
        "img = np.zeros((10*num_show,784))\n",
        "I = y_pred != y_test\n",
        "x_wrong = x_test[I]\n",
        "y_wrong = y_test[I]\n",
        "for i in range(10):\n",
        "    I = y_wrong == i\n",
        "    img[num_show*i:num_show*i + min(num_show,np.sum(I)),:] = x_wrong[I,:][:num_show]\n",
        "\n",
        "gl.utils.image_grid(img,n_rows=10,n_cols=num_show)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise\n",
        "\n",
        "To speed up the training, try training on a much smaller random subset of the training data `x_train`. You can use `np.random.choice` for this. How does the accuracy vary with the size of the training set? Generate some plots to show the relationship between accuracy and size of the training set."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
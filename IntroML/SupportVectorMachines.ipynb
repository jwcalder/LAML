{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support Vector Machines\n",
        "\n",
        "This notebook is a brief introduction to support vector machines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first train SVMs on some toy blob datasets. For this, we will define a function that helps plot the classification decision regions of an SVM (or any classifier for that matter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Generates a plot of the classifier decision regions\n",
        "#corresponding to a classifier trained on the datapoints\n",
        "#in X, with true labels L, and classifier clf\n",
        "#All arguments besides X,L,clf are optional and can be ignored\n",
        "def plot_region(X,L,clf,alpha=0.5,cmap='Paired',cp=np.array([0.5,2.5,6.5,8.5,10.5,11.5]),\n",
        "                markers = ['o','s','D','^','v','p'],vmin=0,vmax=12,markersize=75,\n",
        "                linewidths=1.25,markerlinewidths=1):\n",
        "    x,y = X[:,0],X[:,1]\n",
        "    f = 0.1*np.maximum(np.max(np.abs(x)),np.max(np.abs(y)))\n",
        "    xmin, xmax = np.min(x)-f,np.max(x)+f\n",
        "    ymin, ymax = np.min(y)-f,np.max(y)+f\n",
        "    plt.figure()\n",
        "    for i,color in enumerate(np.unique(cp[L])):\n",
        "        sub = cp[L] == color\n",
        "        plt.scatter(x[sub],y[sub],zorder=2,c=cp[L][sub],cmap=cmap,edgecolors='black',\n",
        "                    vmin=vmin,vmax=vmax,linewidths=markerlinewidths,marker=markers[i],s=markersize)\n",
        "\n",
        "    X,Y = np.mgrid[xmin:xmax:0.01,ymin:ymax:0.01]\n",
        "    z = clf.predict(np.c_[X.ravel(),Y.ravel()]).reshape(X.shape)\n",
        "    plt.contourf(X, Y, cp[z],alpha=alpha,cmap=cmap,antialiased=True,vmin=vmin,vmax=vmax)\n",
        "    plt.contour(X, Y, z, colors='black',linewidths=linewidths,antialiased=True)\n",
        "    plt.xlim((xmin,xmax))\n",
        "    plt.ylim((ymin,ymax))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now generate some blob data, train an SVM classifier using sklearn, and plot the resulting decision regions. For now, we'll use the built-in SVM optimizer in sklearn. In the homwork you'll get a chance to optimize it yourself.\n",
        "\n",
        "Try a diffrenet number of centers (classes) and try different numbers of points. You can also try different kernels ('rbf','poly'). We haven't discussed those yet in class. The random_state allows you to get the same random clusters every time you run the code, so you can compare different kernels on the same data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn import datasets\n",
        "\n",
        "n = 100\n",
        "X,L = datasets.make_blobs(n_samples=n, centers=3)\n",
        "clf = SVC(kernel='linear',decision_function_shape='ovr')\n",
        "clf.fit(X,L)\n",
        "\n",
        "#Plot the region\n",
        "plot_region(X,L,clf)\n",
        "\n",
        "#Print the solution\n",
        "print('w=',clf.coef_)\n",
        "print('b=',clf.intercept_)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now consdier some real data. We will use the breast cancer classification dataset from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = datasets.load_breast_cancer()\n",
        "print(data.DESCR)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below performs a random train/test split, trains an SVM on the training set, and evaluates the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = data.data\n",
        "y = data.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\n",
        "\n",
        "#Train SVM\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "#Training accuracy\n",
        "y_pred = clf.predict(x_train)\n",
        "train_acc = np.mean(y_pred == y_train)*100\n",
        "print('Training Accuracy: %.2f%%'%train_acc)\n",
        "\n",
        "#Testing accuracy\n",
        "y_pred = clf.predict(x_test)\n",
        "test_acc = np.mean(y_pred == y_test)*100\n",
        "print('Testing Accuracy: %.2f%%'%test_acc)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercises\n",
        "\n",
        "1. Play round with different choices of kernels in SVM. The documentation for the function is [here.](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
        "2. As with linear regression, try running SVM over many different random train/test splits and report the average and standard deviation of accuracy. (You can also use a k-fold cross-validation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handwritten digit recognition\n",
        "\n",
        "Here we tackle the problem of handwritten digit recognition with several basic machine learning algorithms. We will first install the [GraphLearning](https://github.com/jwcalder/GraphLearning) Python package, which gives easy access to the MNIST digits dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install graphlearning -q"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now load the MNIST dataset and display some of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "\n",
        "digits,labels = gl.datasets.load('mnist')\n",
        "print(digits.shape)\n",
        "print(labels.shape)\n",
        "gl.utils.image_grid(digits)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The standard train/test split for MNIST is to take the first 60000 images as training, and the last 10000 as testing. Let's run SVM and k-nearest neighbors on this train/test split. The code will take a long time to run (5-10 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "#standard train/test split\n",
        "x_train = digits[:60000,:]\n",
        "x_test = digits[60000:,:]\n",
        "y_train = labels[:60000]\n",
        "y_test = labels[60000:]\n",
        "\n",
        "#Train SVM\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "#Training accuracy\n",
        "y_pred = clf.predict(x_train)\n",
        "train_acc = np.mean(y_pred == y_train)*100\n",
        "print('SVM Training Accuracy: %.2f%%'%train_acc)\n",
        "\n",
        "#Testing accuracy\n",
        "y_pred = clf.predict(x_test)\n",
        "test_acc = np.mean(y_pred == y_test)*100\n",
        "print('SVM Testing Accuracy: %.2f%%'%test_acc)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot some of the misclassified images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_show = 20\n",
        "img = np.zeros((10*num_show,784))\n",
        "I = y_pred != y_test\n",
        "x_wrong = x_test[I]\n",
        "y_wrong = y_test[I]\n",
        "for i in range(10):\n",
        "    I = y_wrong == i\n",
        "    img[num_show*i:num_show*i + min(num_show,np.sum(I)),:] = x_wrong[I,:][:num_show]\n",
        "\n",
        "gl.utils.image_grid(img,n_rows=10,n_cols=num_show)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise\n",
        "\n",
        "To speed up the training, try training on a much smaller random subset of the training data `x_train`. You can use `np.random.choice` for this. How does the accuracy vary with the size of the training set? Generate some plots to show the relationship between accuracy and size of the training set."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Binary Spectral Clustering\n",
        "\n",
        "We give some examples of binary spectral clustering here on toy and real-world data sets. First we install [GraphLearning](https://github.com/jwcalder/GraphLearning) and [Annoy](https://github.com/spotify/annoy) (for nearest neighbor searches in graphlearning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install -q annoy graphlearning"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We implement the k-means algorithm below, to compare against spectral clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def kmeans(X,k,plot_clustering=False,T=200):\n",
        "    \"\"\"\n",
        "    k-means Clustering\n",
        "\n",
        "    Args:\n",
        "        X: nxm array of data, each row is a datapoint\n",
        "        k: Number of clusters\n",
        "        plot_clustering: Whether to plot final clustering\n",
        "        T: Max number of iterations\n",
        "\n",
        "    Returns:\n",
        "        Numpy array of labels obtained by binary k-means clustering\n",
        "    \"\"\"\n",
        "\n",
        "    #Number of data points\n",
        "    n = X.shape[0]\n",
        "\n",
        "    #Randomly choose initial cluster means\n",
        "    means = X[np.random.choice(n,size=k,replace=False),:]\n",
        "\n",
        "    #Initialize arrays for distances and labels\n",
        "    dist = np.zeros((k,n))\n",
        "    labels = np.zeros((n,))\n",
        "\n",
        "    #Main iteration for kmeans\n",
        "    num_changed = 1\n",
        "    i=0\n",
        "    while i < T and num_changed > 0:\n",
        "\n",
        "        #Update labels\n",
        "        old_labels = labels.copy()\n",
        "        for j in range(k):\n",
        "            dist[j,:] = np.sum((X - means[j,:])**2,axis=1)\n",
        "        labels = np.argmin(dist,axis=0)\n",
        "        num_changed = np.sum(labels != old_labels)\n",
        "\n",
        "        #Update means\n",
        "        for j in range(k):\n",
        "            means[j,:] = np.mean(X[labels==j,:],axis=0)\n",
        "\n",
        "        #Iterate counter\n",
        "        i+=1\n",
        "\n",
        "        #Plot result (red points are labels)\n",
        "    if plot_clustering:\n",
        "        plt.scatter(X[:,0],X[:,1], c=labels)\n",
        "        plt.scatter(means[:,0],means[:,1], c='r')\n",
        "        plt.title('K-means clustering')\n",
        "\n",
        "    return labels, means"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now test the algorithm on the two-moons and circles datasets, which k-means does poorly on. Uncomment the make_circles line to try that dataset instead of two moons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sklearn.datasets as datasets\n",
        "import graphlearning as gl\n",
        "\n",
        "n=300 #Number of data points\n",
        "\n",
        "#Two Moons or make circles\n",
        "X,L = datasets.make_moons(n_samples=n,noise=0.1)\n",
        "#X,L = datasets.make_circles(n_samples=n,noise=0.075,factor=0.5)\n",
        "labels, means = kmeans(X,2)\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1],c=labels)\n",
        "plt.scatter(means[:,0],means[:,1],c='red',marker='*',s=200)\n",
        "plt.title('k-means')\n",
        "\n",
        "#Build graph and draw\n",
        "W = gl.weightmatrix.epsilon_ball(X,0.25)\n",
        "G = gl.graph(W)\n",
        "G.draw(X=X,c=L,linewidth=0.1)\n",
        "plt.title('Graph')\n",
        "\n",
        "#Fiedler vector\n",
        "v = G.fiedler_vector()\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1],c=v)\n",
        "plt.title('Fiedler Vector')\n",
        "\n",
        "#Spectral clustering\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1],c=v>0)\n",
        "plt.title('Spectral Clustering')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now try specral clustering on a real-world graph. We'll use Zachary's karate club graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "G = gl.datasets.load_graph('karate')\n",
        "L = G.labels\n",
        "v = G.fiedler_vector()\n",
        "v = -v*v[0] #Makes v[0] negative, to fix a consistent sign for the Fiedler vector,\n",
        "print('True Labels        ',L)\n",
        "print('Spectral Clustering',(v > 0).astype(int))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spectral clustering is only wrong in the classification of two members of the Karate club, members 3 and 9. To see if we can glean any more information from the Fiedler vector, we sort the club members by the value of the Fiedler vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ind = np.argsort(v)\n",
        "plt.figure()\n",
        "plt.scatter(range(34),v[ind],c=L[ind])\n",
        "plt.ylabel('Fiedler vector value')\n",
        "plt.xlabel('Sorted member number')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspecting the plot, we see that the Fiedler vector does perfectly separate the two groups if we threshold at a value slightly below zero. Inspecting the plot, and a bit of trial and error, yields $0.007$ as a good threshold. If you knew the desired sizes of the two groups, could you find an automatic way to select this threshold?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('True Labels        ',L)\n",
        "print('Spectral Clustering',(v > 0.007).astype(int))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Try binary spectral clustering on another real-world graph from the [graphlearning package](https://jwcalder.github.io/GraphLearning/datasets.html#graphlearning.datasets.load_graph) For example, `polbooks` is similarly small and easy to work with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral clustering on MNIST\n",
        "\n",
        "We now experiment with clustering MNIST digits using binary spectral clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "\n",
        "#Load MNIST data and labels and plot some images\n",
        "data, labels = gl.datasets.load('MNIST')\n",
        "gl.utils.image_grid(data, n_rows=20, n_cols=20, title='Some MNIST Images', fontsize=26)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below clusters a pair of MNIST digits. Try different pairs, which are hardest to separate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "#Subset data to two digits and convert labels to zeros/ones\n",
        "digits = (3,8)\n",
        "I,J = labels == digits[0], labels == digits[1]\n",
        "X,L = data[I | J,:], (labels[I | J] == digits[1]).astype(int)\n",
        "\n",
        "#Spectral Clustering (sparse 10-nearest neighbor graph)\n",
        "W = gl.weightmatrix.knn(X,10)\n",
        "G = gl.graph(W)\n",
        "spectral_labels = (G.fiedler_vector() > 0).astype(int)\n",
        "acc1 = np.mean(spectral_labels == L)\n",
        "acc2 = np.mean(spectral_labels != L)\n",
        "print('Spectral clustering accuracy = %.2f%%'%(100*max(acc1,acc2)))\n",
        "\n",
        "#k-means clustering\n",
        "kmeans_labels, means = kmeans(X,2)\n",
        "acc1 = np.mean(kmeans_labels == L)\n",
        "acc2 = np.mean(kmeans_labels != L)\n",
        "print('K-means clustering accuracy = %.2f%%'%(100*max(acc1,acc2)))\n",
        "\n",
        "#Show images from each cluster\n",
        "gl.utils.image_grid(X[spectral_labels==0,:], n_rows=10, n_cols=10, title='Cluster 1', fontsize=26)\n",
        "gl.utils.image_grid(X[spectral_labels==1,:], n_rows=10, n_cols=10, title='Cluster 2', fontsize=26)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Implement the spectral approach to community detection via modularity maximization described in the book. The method is similar to spectral clustering, in that it uses the second eigenvector of a matrix similar to the graph Laplacian (the modularity matrix). How does modularity compare to spectral clustering on the examples in this notebook?"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#t-SNE embedding\n",
        "\n",
        "This notebook explores the t-SNE embedding method for visualizing high dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install graphlearning annoy"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is code for implementing t-SNE from scratch. This only works on small data sets, but is useful for understanding how the algorithm works and playing around with the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def perp(p):\n",
        "    \"Perplexity\"\n",
        "\n",
        "    p = p + 1e-10\n",
        "    return 2**(-np.sum(p*np.log2(p),axis=1))\n",
        "\n",
        "def pmatrix(X,sigma):\n",
        "    \"P matrix in t-SNE\"\n",
        "\n",
        "    n = len(sigma)\n",
        "    I = np.zeros((n,n), dtype=int)+np.arange(n, dtype=int)\n",
        "    dist = np.sum((X[I,:] - X[I.T,:])**2,axis=2)\n",
        "    W = np.exp(-dist/(2*sigma[:,np.newaxis]**2))\n",
        "    W[range(n),range(n)]=0\n",
        "    deg = W@np.ones(n)\n",
        "    return np.diag(1/deg)@W   #P matrix for t-SNE\n",
        "\n",
        "def bisect(X,perplexity):\n",
        "    \"Bisection search to find sigma for a given perplexity\"\n",
        "\n",
        "    m = X.shape[0]\n",
        "    sigma = np.ones(m)\n",
        "    P = pmatrix(X,sigma)\n",
        "    while np.min(perp(P)) < perplexity:\n",
        "        sigma *= 2\n",
        "        P = pmatrix(X,sigma)\n",
        "\n",
        "    #bisection search\n",
        "    sigma1 = np.zeros_like(sigma)\n",
        "    sigma2 = sigma.copy()\n",
        "    for i in range(20):\n",
        "        sigma = (sigma1+sigma2)/2\n",
        "        P = pmatrix(X,sigma)\n",
        "        K = perp(P) > perplexity\n",
        "        sigma2 = sigma*K + sigma2*(1-K)\n",
        "        sigma1 = sigma1*K + sigma*(1-K)\n",
        "\n",
        "    return sigma\n",
        "\n",
        "def GL(W):\n",
        "    \"Returns Graph Laplacian for weight matrix W\"\n",
        "    deg = W@np.ones(W.shape[0])\n",
        "    return np.diag(deg) - W\n",
        "\n",
        "def tsne(X,perplexity=50,h=1,alpha=50,num_early=100,num_iter=1000):\n",
        "    \"\"\"t-SNE embedding\n",
        "\n",
        "    Args:\n",
        "        X: Data cloud\n",
        "        perplexity: Perplexity (roughly how many neighbors to use)\n",
        "        h: Time step\n",
        "        alpha: Early exaggeration factor\n",
        "        num_early: Number of early exaggeration steps\n",
        "        num_iter: Total number of iterations\n",
        "\n",
        "    Returns:\n",
        "        Y: Embedded points\n",
        "    \"\"\"\n",
        "\n",
        "    #Build graph using perplexity\n",
        "    m = X.shape[0]\n",
        "    sigma = bisect(X,perplexity)\n",
        "    P = pmatrix(X,sigma)\n",
        "    P = (P.T + P)/(2*m)\n",
        "\n",
        "    #For indexing\n",
        "    I = np.zeros((m,m), dtype=int)+np.arange(m, dtype=int)\n",
        "\n",
        "    #Initialization\n",
        "    Y = np.random.rand(X.shape[0],2)\n",
        "\n",
        "    #Main gradient descent loop\n",
        "    for i in range(num_iter):\n",
        "\n",
        "        #Compute embedded matrix Q\n",
        "        q = 1/(1+np.sum((Y[I,:] - Y[I.T,:])**2,axis=2))\n",
        "        q[range(m),range(m)]=0\n",
        "        Z = np.sum(q)\n",
        "        Q = q/Z\n",
        "\n",
        "        #Compute gradient\n",
        "        if i < num_early: #Early exaggeration\n",
        "            grad = 4*Z*(alpha*GL(P*Q) - GL(Q**2))@Y\n",
        "        else:\n",
        "            grad = 4*Z*GL((P-Q)*Q)@Y\n",
        "\n",
        "        #Gradient descent\n",
        "        Y -= h*grad\n",
        "\n",
        "        #Percent complete\n",
        "        if i % int(num_iter/10) == 0:\n",
        "            print('%d%%'%(int(100*i/num_iter)))\n",
        "\n",
        "    return Y,P"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try the t-SNE algorithm on a subset of the MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "import numpy as np\n",
        "\n",
        "#Load MNIST labels and results of k-nearest neighbor search\n",
        "data, labels = gl.datasets.load('MNIST')\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "#Display some random MNIST images\n",
        "gl.utils.image_grid(data[np.random.permutation(data.shape[0])],n_rows=20,n_cols=20)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implementation is for illustration and is in particular not sparse. So we can only run this on relatively small datasets. We run it on 300 images from MNIST below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import graphlearning as gl\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Load MNIST data and labels\n",
        "data, labels = gl.datasets.load('mnist')\n",
        "\n",
        "#Subsample MNIST\n",
        "sz = 1000\n",
        "X = data[labels <= 4]\n",
        "T = labels[labels <= 4]\n",
        "sub = np.random.choice(len(T),size=sz)\n",
        "X = X[sub,:]\n",
        "T = T[sub]\n",
        "\n",
        "#Run PCA first\n",
        "pca = PCA(n_components=50)\n",
        "X = pca.fit_transform(X)\n",
        "\n",
        "#Run t-SNE\n",
        "Y,P = tsne(X,perplexity=30,h=sz,alpha=10,num_early=100,num_iter=500)\n",
        "\n",
        "#Create scatterplot of embedding\n",
        "plt.figure()\n",
        "plt.scatter(Y[:,0],Y[:,1],c=T)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sklearn implementation of t-SNE uses a faster implementation that can handle larger data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import graphlearning as gl\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Load MNIST data and labels\n",
        "data, labels = gl.datasets.load('mnist')\n",
        "\n",
        "#Subsample MNIST\n",
        "sub = np.random.choice(len(labels),size=5000)\n",
        "X = data[sub,:]\n",
        "T = labels[sub]\n",
        "\n",
        "#Run PCA first\n",
        "pca = PCA(n_components=50)\n",
        "X = pca.fit_transform(X)\n",
        "\n",
        "#Run t-SNE\n",
        "Y = TSNE(n_components=2, perplexity=30).fit_transform(X)\n",
        "\n",
        "#Create scatterplot of embedding\n",
        "plt.figure()\n",
        "plt.scatter(Y[:,0],Y[:,1],c=T,s=0.5)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we show the t-SNE embedding of a parabola in 10 dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 1000\n",
        "X = np.zeros((n,10))\n",
        "X[:,0] = np.linspace(-1,1,n)\n",
        "X[:,1] = X[:,0]**2\n",
        "X_tsne = TSNE(n_components=2, perplexity=20).fit_transform(X)\n",
        "plt.figure()\n",
        "plt.scatter(X_tsne[:,0],X_tsne[:,1],s=2,c=X[:,0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "1. Run t-SNE on the two moons data set or the circles data set for different values of perplexity, to reproduce the results from the textbook.\n",
        "2. Try the t-SNE algorithm on a k-nearest neighbor graph, instead of the perplexity graph construction. You will have to modify the provided t-SNE code to do this. Can you get similar results?"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
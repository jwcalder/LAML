{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph Convolutional Neural Networks\n",
        "\n",
        "This notebook is a brief introduction to graph convolutional neural networks (GCN) in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install -q graphlearning"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a simple GCN, as well as its MLP counterpart that does not use graph information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import graphlearning as gl\n",
        "from scipy import sparse\n",
        "\n",
        "def csr_to_torch(W):\n",
        "\n",
        "    W = W.tocoo()\n",
        "    values = W.data\n",
        "    indices = np.vstack((W.row, W.col))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = W.shape\n",
        "\n",
        "    return torch.sparse_coo_tensor(i, v, shape)\n",
        "\n",
        "class diffusionGCN(nn.Module):\n",
        "    def __init__(self, G, num_hidden=30, k=2):\n",
        "        super(diffusionGCN, self).__init__()\n",
        "        num_in = G.features.shape[1]\n",
        "        num_classes = len(np.unique(G.labels))\n",
        "        self.fc1 = nn.Linear(num_in,num_hidden)\n",
        "        self.fc2 = nn.Linear(num_hidden,num_hidden)\n",
        "        self.fc3 = nn.Linear(num_hidden,num_classes)\n",
        "\n",
        "        #Weights for diffusion\n",
        "        self.k = k\n",
        "        self.w = nn.Parameter(torch.randn(self.k+1))\n",
        "\n",
        "        #Renormalization trick\n",
        "        W = G.weight_matrix\n",
        "        H = gl.graph(W + sparse.eye(G.num_nodes))\n",
        "        D = H.degree_matrix(p=-0.5)\n",
        "        A = D*H.weight_matrix*D\n",
        "        self.A = csr_to_torch(A).to(device)\n",
        "\n",
        "    def convolution(self, x):\n",
        "        y = self.w[0]*x\n",
        "        for i in range(1,self.k+1):\n",
        "            x = self.A@x\n",
        "            y += self.w[i]*x\n",
        "        return y\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.convolution(self.fc1(x)))\n",
        "        x = F.relu(self.convolution(self.fc2(x)))\n",
        "        x = F.log_softmax(self.fc3(x),dim=1)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = F.relu(self.convolution(self.fc1(x)))\n",
        "        return self.convolution(self.fc2(x))\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, G, num_hidden=30):\n",
        "        super(GCN, self).__init__()\n",
        "        num_in = G.features.shape[1]\n",
        "        num_classes = len(np.unique(G.labels))\n",
        "        self.fc1 = nn.Linear(num_in,num_hidden)\n",
        "        self.fc2 = nn.Linear(num_hidden,num_hidden)\n",
        "        self.fc3 = nn.Linear(num_hidden,num_classes)\n",
        "\n",
        "        #Renormalization trick\n",
        "        W = G.weight_matrix\n",
        "        H = gl.graph(W + sparse.eye(G.num_nodes))\n",
        "        D = H.degree_matrix(p=-1)\n",
        "        A = D*H.weight_matrix\n",
        "        self.A = csr_to_torch(A).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.A@self.fc1(x))\n",
        "        x = F.relu(self.A@self.fc2(x))\n",
        "        x = F.log_softmax(self.fc3(x),dim=1)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = F.relu(self.A@self.fc1(x))\n",
        "        x = self.A@self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, G, num_hidden=30):\n",
        "        super(MLP, self).__init__()\n",
        "        num_in = G.features.shape[1]\n",
        "        num_classes = len(np.unique(G.labels))\n",
        "        self.fc1 = nn.Linear(num_in,num_hidden)\n",
        "        self.fc2 = nn.Linear(num_hidden,num_hidden)\n",
        "        self.fc3 = nn.Linear(num_hidden,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return F.log_softmax(self.fc3(x),dim=1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now run a semi-supervised learning trial on the PubMed data set with GCN compared to MLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import graphlearning as gl\n",
        "import torch.optim as optim\n",
        "from scipy import sparse\n",
        "\n",
        "#Load Graph\n",
        "G = gl.datasets.load_graph('pubmed')\n",
        "W = G.weight_matrix\n",
        "labels = G.labels\n",
        "X = G.features\n",
        "m,n = X.shape\n",
        "\n",
        "np.random.seed(1) #For reproducibility\n",
        "train_ind = gl.trainsets.generate(labels, rate=0.003)\n",
        "train_labels = labels[train_ind]\n",
        "train_mask = np.zeros(m,dtype=bool)\n",
        "train_mask[train_ind]=True\n",
        "test_mask = ~train_mask\n",
        "test_ind = np.arange(m)[test_mask].astype(int)\n",
        "\n",
        "#GPU\n",
        "gpu = True\n",
        "if gpu:\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#Setup model and initial plot\n",
        "model_GCN = GCN(G,num_hidden=100).to(device)\n",
        "model_diffusionGCN = diffusionGCN(G,num_hidden=100,k=3).to(device)\n",
        "model_MLP = MLP(G,num_hidden=100).to(device)\n",
        "optimizer_GCN = optim.Adam(model_GCN.parameters(), lr=0.01)  #Learning rates\n",
        "optimizer_diffusionGCN = optim.Adam(model_diffusionGCN.parameters(), lr=0.01)  #Learning rates\n",
        "optimizer_MLP = optim.Adam(model_MLP.parameters(), lr=0.01)  #Learning rates\n",
        "\n",
        "#Convert data to torch and device\n",
        "data = torch.from_numpy(X).float().to(device)\n",
        "target = torch.from_numpy(labels).long().to(device)\n",
        "train_set = torch.from_numpy(train_ind).long().to(device)\n",
        "test_set = torch.from_numpy(test_ind).long().to(device)\n",
        "\n",
        "print('Iteration,GCN,Diffusion GCN, MLP')\n",
        "for t in range(100):\n",
        "\n",
        "    optimizer_GCN.zero_grad()\n",
        "    output_GCN = model_GCN(data)\n",
        "    loss = F.nll_loss(output_GCN[train_set,:], target[train_set])\n",
        "    loss.backward()\n",
        "    optimizer_GCN.step()\n",
        "\n",
        "    optimizer_diffusionGCN.zero_grad()\n",
        "    output_diffusionGCN = model_diffusionGCN(data)\n",
        "    loss = F.nll_loss(output_diffusionGCN[train_set,:], target[train_set])\n",
        "    loss.backward()\n",
        "    optimizer_diffusionGCN.step()\n",
        "\n",
        "    optimizer_MLP.zero_grad()\n",
        "    output_MLP = model_MLP(data)\n",
        "    loss = F.nll_loss(output_MLP[train_set,:], target[train_set])\n",
        "    loss.backward()\n",
        "    optimizer_MLP.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = torch.argmax(output_GCN,axis=1)\n",
        "        GCN_accuracy = 100*torch.sum(pred[test_set] == target[test_set])/len(test_set)\n",
        "        pred = torch.argmax(output_diffusionGCN,axis=1)\n",
        "        diffusionGCN_accuracy = 100*torch.sum(pred[test_set] == target[test_set])/len(test_set)\n",
        "        pred = torch.argmax(output_MLP,axis=1)\n",
        "        MLP_accuracy = 100*torch.sum(pred[test_set] == target[test_set])/len(test_set)\n",
        "        print('%d,%f,%f,%f'%(t,GCN_accuracy,diffusionGCN_accuracy,MLP_accuracy),flush=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now look at GCN node embeddings for the karate and political books data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Load Graph and set features to identity matrix\n",
        "graph = 'karate' #or 'polbooks'\n",
        "G = gl.datasets.load_graph(graph)\n",
        "W = G.weight_matrix\n",
        "if graph == 'polbooks':\n",
        "    labels = (G.fiedler_vector() > 0).astype(int)\n",
        "else:\n",
        "    labels = G.labels\n",
        "m = G.num_nodes\n",
        "n = m\n",
        "X = np.eye(m)\n",
        "G.features = X\n",
        "\n",
        "np.random.seed(1) #For reproducibility\n",
        "train_ind = gl.trainsets.generate(labels, rate=1)\n",
        "train_labels = labels[train_ind]\n",
        "train_mask = np.zeros(m,dtype=bool)\n",
        "train_mask[train_ind]=True\n",
        "test_mask = ~train_mask\n",
        "test_ind = np.arange(m)[test_mask].astype(int)\n",
        "\n",
        "#Setup model and initial plot\n",
        "device = torch.device(\"cpu\")\n",
        "model = diffusionGCN(G,num_hidden=50)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  #Learning rates\n",
        "\n",
        "#Convert data to torch and device\n",
        "data = torch.from_numpy(X).float()\n",
        "target = torch.from_numpy(labels).long()\n",
        "train_set = torch.from_numpy(train_ind).long()\n",
        "test_set = torch.from_numpy(test_ind).long()\n",
        "\n",
        "for t in range(100):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output[train_set,:], target[train_set])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = torch.argmax(output,axis=1)\n",
        "        test_accuracy = 100*torch.sum(pred[test_set] == target[test_set])/len(test_set)\n",
        "        print('%d,%f'%(t,test_accuracy),flush=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    Y = model.encode(data).numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    Z = pca.fit_transform(Y)\n",
        "    G.draw(X=Z,c=G.labels,linewidth=0.5)\n",
        "    plt.scatter(Z[train_ind,0],Z[train_ind,1],c='red',marker='o',s=10,zorder=100)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
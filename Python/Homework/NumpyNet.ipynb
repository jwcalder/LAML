{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2-layer neural net in Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install -q graphlearning"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "import numpy as np\n",
        "\n",
        "#Softmax on batches\n",
        "def softmax(x):\n",
        "    p = np.exp(x)\n",
        "    norms = np.linalg.norm(p,axis=1,ord=1)\n",
        "    return p/norms[:,None]\n",
        "\n",
        "#Softmax followed by negative log likelihood loss\n",
        "#Also returns gradient v\n",
        "def softmax_nll(x,y):\n",
        "    k = np.arange(x.shape[0])\n",
        "    v = softmax(x)\n",
        "    v[k,y] -= 1\n",
        "    mx = np.max(x,axis=1)\n",
        "    loss = -x[k,y] + np.log(np.linalg.norm(np.exp(x - mx[:,None]),axis=1,ord=1))\n",
        "    loss = np.mean(loss)\n",
        "    return v,loss\n",
        "\n",
        "#Basic neural net class in Numpy\n",
        "class NumpyNet():\n",
        "    def __init__(self, num_in, num_hidden, num_out):\n",
        "        self.W1 = np.random.randn(num_hidden,num_in)\n",
        "        self.b1 = np.random.randn(num_hidden,1)\n",
        "        self.W2 = np.random.randn(num_out,num_hidden)\n",
        "        self.b2 = np.random.randn(num_out,1)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    #Forward propagation\n",
        "    def forward(self, x):\n",
        "        self.x = x.T\n",
        "        self.p1 = self.W1@self.x + self.b1\n",
        "        self.z1 = np.maximum(self.p1,0) #ReLU activation\n",
        "        self.p2 = self.W2@self.z1+self.b2\n",
        "        self.z2 = self.p2\n",
        "        return self.z2.T\n",
        "\n",
        "    #Zero gradients\n",
        "    def zero_grad(self):\n",
        "        self.grad_W1 = np.zeros_like(self.W1)\n",
        "        self.grad_b1 = np.zeros_like(self.b1)\n",
        "        self.grad_W2 = np.zeros_like(self.W2)\n",
        "        self.grad_b2 = np.zeros_like(self.b2)\n",
        "\n",
        "    #Back propagation\n",
        "    def backward(self,v):\n",
        "        self.zero_grad()\n",
        "        bs,d = v.shape\n",
        "\n",
        "        #Loop over batch to avoid tensors\n",
        "        for i in range(bs):\n",
        "            v2 = v[[i],:].T\n",
        "            z1 = self.z1[:,[i]]\n",
        "            p1 = self.p1[:,[i]]\n",
        "            x = self.x[:,[i]]\n",
        "\n",
        "            #Layer 2\n",
        "            self.grad_b2 += v2\n",
        "            self.grad_W2 += v2@z1.T\n",
        "\n",
        "            #Back propagate\n",
        "            v1 = self.W2.T@v2\n",
        "\n",
        "            #Layer 1\n",
        "            S1 = np.diag(p1.flatten() >= 0)\n",
        "            self.grad_b1 += S1@v1\n",
        "            self.grad_W1 += S1@self.W2.T@v2@x.T\n",
        "\n",
        "        self.grad_W1 /= bs\n",
        "        self.grad_b1 /= bs\n",
        "        self.grad_W2 /= bs\n",
        "        self.grad_b2 /= bs\n",
        "\n",
        "    #Optimization step\n",
        "    def step(self,lr):\n",
        "        self.W1 -= lr*self.grad_W1\n",
        "        self.W2 -= lr*self.grad_W2\n",
        "        self.b1 -= lr*self.grad_b1\n",
        "        self.b2 -= lr*self.grad_b2\n",
        "\n",
        "#Load MNIST data\n",
        "x,y = gl.datasets.load('mnist')\n",
        "\n",
        "num_hidden = 64\n",
        "batch_size = 128\n",
        "\n",
        "#Training data (select at random from first 600000)\n",
        "train_size = 60000\n",
        "train_ind = np.random.permutation(60000)[:train_size]\n",
        "\n",
        "#Convert data to torch and device\n",
        "data_train = x[train_ind,:]\n",
        "target_train = y[train_ind]\n",
        "data_test = x[60000:,:]\n",
        "target_test = y[60000:]\n",
        "\n",
        "#Setup model and optimizer\n",
        "model = NumpyNet(784,num_hidden,10)\n",
        "lr = 1\n",
        "\n",
        "#Training\n",
        "print('Iteration,Testing Accuracy,Training Accuracy')\n",
        "for i in range(100):\n",
        "\n",
        "    #Test model\n",
        "    pred = np.argmax(model(data_test),axis=1)\n",
        "    test_accuracy = np.mean(pred == target_test)\n",
        "    pred = np.argmax(model(data_train),axis=1)\n",
        "    train_accuracy = np.mean(pred == target_train)\n",
        "    print(i,test_accuracy*100,train_accuracy*100)\n",
        "\n",
        "    #Training mode, run data through neural network in mini-batches (SGD)\n",
        "    for j in range(0,len(target_train),batch_size):\n",
        "        xb,yb = data_train[j:j+batch_size,:], target_train[j:j+batch_size]\n",
        "        v,loss = softmax_nll(model(xb),yb)\n",
        "        model.backward(v)\n",
        "        model.step(lr)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
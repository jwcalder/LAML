{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Discriminant Analysis (LDA)\n",
        "\n",
        "This notebook gives a brief introduction to Linear Discriminant Analysis (LDA). Let us first define some helper functions that will compute LDA and PCA for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "#Computes components (discriminating directions) for LDA\n",
        "def lda_comp(X,labels,k=2,lam=1e-10):\n",
        "\n",
        "    within_class,between_class = lda_cov(X,labels)\n",
        "    within_class += lam*np.eye(X.shape[1])\n",
        "    vals,V = sparse.linalg.eigsh(between_class,M=within_class,k=k,which='LM')\n",
        "    V = V[:,::-1]\n",
        "    vals = vals[::-1]\n",
        "    return V\n",
        "\n",
        "#LDA projection\n",
        "def lda(X,labels,k=2,lam=1e-10):\n",
        "\n",
        "    V = lda_comp(X,labels,k=k,lam=lam) \n",
        "    return X@V\n",
        "\n",
        "\n",
        "#Computes principal components\n",
        "def pca_comp(X,k=2):\n",
        "\n",
        "    M = (X - np.mean(X,axis=0)).T@(X - np.mean(X,axis=0))\n",
        "\n",
        "    #Use eigsh to get subset of eigenvectors \n",
        "    vals, V = sparse.linalg.eigsh(M, k=k, which='LM')\n",
        "    V = V[:,::-1]\n",
        "    vals = vals[::-1]\n",
        "\n",
        "    return vals,V\n",
        "\n",
        "#PCA projection\n",
        "def pca(X,k=2,whiten=False):\n",
        "\n",
        "    vals,V = pca_comp(X,k=k)\n",
        "\n",
        "    #Now project X onto the 2-D subspace spanned by \n",
        "    #computing the 2D PCA coorindates of each point in X\n",
        "    X_pca = X@V\n",
        "    if whiten:\n",
        "        print('whiten')\n",
        "        S = np.diag(vals**(-1/2))\n",
        "        X_pca = X_pca@S\n",
        "\n",
        "    return X_pca\n",
        "\n",
        "\n",
        "#LDA covariance matrices\n",
        "def lda_cov(X,labels):\n",
        "    num_classes = np.max(labels)+1\n",
        "    within_class = np.zeros((X.shape[1],X.shape[1]))\n",
        "    means = []\n",
        "    counts = []\n",
        "    for i in range(num_classes):\n",
        "        Xs = X[labels==i,:].copy()\n",
        "        counts += [np.sum(labels==i)]\n",
        "        m = np.mean(Xs,axis=0)\n",
        "        means += [m]\n",
        "        within_class += (Xs-m).T@(Xs-m)\n",
        "\n",
        "    means = np.array(means)\n",
        "    counts = np.array(counts)\n",
        "    Y = (means - np.mean(X,axis=0))*np.sqrt(counts[:,None])\n",
        "    between_class = Y.T@Y\n",
        "\n",
        "    return within_class, between_class"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first consider a toy data set in three dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Toy data\n",
        "n = 1000\n",
        "mean = [0,0,0]\n",
        "cov = [[0.1,0,0], [0,1,0], [0,0,1]]  \n",
        "X = np.random.multivariate_normal(mean, cov, n)\n",
        "Y = np.random.multivariate_normal(mean, cov, n) + np.array([1,0,0])\n",
        "X = np.vstack((X,Y))\n",
        "L = np.hstack((np.zeros(n),np.ones(n))).astype(int)\n",
        "\n",
        "#PCA\n",
        "Y = pca(X)\n",
        "plt.figure()\n",
        "plt.title('PCA')\n",
        "plt.scatter(Y[:,0],Y[:,1],c=L,s=10,vmin=0,vmax=2)\n",
        "\n",
        "#LDA\n",
        "Y = lda(X,L)\n",
        "plt.figure()\n",
        "plt.title('LDA')\n",
        "plt.scatter(Y[:,0],Y[:,1],c=L,s=10,vmin=0,vmax=2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now run this on MNIST and compare to PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install -q graphlearning"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Load MNIST data and subset to a random selection of 5000 images\n",
        "data, labels = gl.datasets.load('mnist')\n",
        "ind = np.random.choice(data.shape[0],size=5000)\n",
        "data = data[ind,:]\n",
        "labels = labels[ind]\n",
        "\n",
        "#Subset to a smaller number of digits\n",
        "num = 5   #Number of digits to use\n",
        "X = data[labels < num] #subset to 0s and 1s\n",
        "L = labels[labels < num] #corresponding labels\n",
        "\n",
        "#PCA\n",
        "Y = pca(X)\n",
        "plt.figure()\n",
        "plt.title('PCA')\n",
        "plt.scatter(Y[:,0],Y[:,1],c=L,s=10)\n",
        "\n",
        "#LDA\n",
        "Y = lda(X,L)\n",
        "plt.figure()\n",
        "plt.title('LDA')\n",
        "plt.scatter(Y[:,0],Y[:,1],c=L,s=10)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Try another data set in graphlearning, like 'fashionmnist'.\n",
        "2. Use LDA as preprocessing for classification via support vector machines (SVM). Try MNIST, FashionMNIST or a data set from sklearn. Make sure to train LDA only on the training data.\n",
        "3. Rewrite the LDA code so that instead of using covariance shrinkage, we project the data onto the top principal components so that the within covariance matrix is nonsingular.\n",
        "4. Similar to 3, rewrite the LDA code to use the method in Exercise 4.2 in the LDA section of the course textbook."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
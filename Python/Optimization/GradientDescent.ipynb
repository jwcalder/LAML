{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Gradient Descent\n",
        "\n",
        "This notebook gives a gentle introduction to gradient descent.\n",
        "\n",
        "First define the function you wish to minimize, and its gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f(x):\n",
        "    return x[0]**2 + 4*x[1]**2 + x[0]*x[1]\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([2*x[0] + x[1], 8*x[1] + x[0]])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function we defined is\n",
        "$$f(x,y) = x^2 + 4y^2 + xy,$$\n",
        "whose gradient is\n",
        "$$\\nabla f(x,y) = (2x + y,8y + x)^T.$$\n",
        "This is a quadratic function with a global minimum at $(0,0)$, with value $f(0,0)=0$. We will now minmimize this function with gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1.,1.])  #initial condition\n",
        "num_steps = 100\n",
        "alpha = 0.2\n",
        "\n",
        "f_vals = np.zeros(num_steps)\n",
        "dist_vals = np.zeros(num_steps)\n",
        "\n",
        "for i in range(num_steps):\n",
        "    x -= alpha*grad_f(x)\n",
        "    f_vals[i] = f(x)\n",
        "    print(\"Iteration: \",i,': f(x) =',f(x))\n",
        "    dist_vals[i] = np.linalg.norm(x)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see if/how it worked, let's plot the energy and distance to the minimizer over each step of gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(f_vals, label='f(x_k)')\n",
        "plt.plot(dist_vals, label='||x_k||')\n",
        "plt.xlabel('Number of steps (k)', fontsize=16)\n",
        "plt.legend(fontsize=16)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to see a convergence rate, we should plot on a log scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(f_vals, label='f(x_k)')\n",
        "plt.plot(dist_vals, label='||x_k||')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Number of steps', fontsize=16)\n",
        "plt.legend(fontsize=16)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check the convergence rate, we can find a line of best fit in the log scale. That is, we expect the error curve is $y = C\\mu^k$ for some $C>0$ and $0 < \\mu < 1$, where $k$ is the iteration index. To find the convergence rate $\\mu$, we take $\\log$ on both sides to get $\\log(y) = \\log(C) + \\log(\\mu)k$. Thus, we can find a line of best fit of the form $\\log(y) = b + mk$, and then $\\mu = e^{m}$. \n",
        "\n",
        "The code below does this, and we see a rate of $0.4$ for $f(x_k)$, which means the error is reduced by a factor of $0.4$ at each iteration. This rate is the square of the rate for $\\|x_k\\|$, since $f$ is quadratic near the minimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "k = np.arange(len(f_vals))\n",
        "m,b = np.polyfit(k,np.log(f_vals),1)\n",
        "mu = np.exp(m)\n",
        "print('Convergence rate for f(x_k) (mu) = ', mu)\n",
        "\n",
        "m,b = np.polyfit(k,np.log(dist_vals),1)\n",
        "mu = np.exp(m)\n",
        "print('Convergence rate for x_k (mu) = ', mu)\n",
        "print('mu^2=',mu**2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also visualize the path taken by gradient descent. Try playing around with the time step $\\alpha$ and see how the path changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x,y = 1,1 #Initial values of x=1 and y=1\n",
        "\n",
        "plt.text(x,y,f'Initial point=({x},{y})',horizontalalignment='right')\n",
        "alpha = 0.2\n",
        "N  = 20\n",
        "for i in range(N):\n",
        "    plt.scatter(x,y,c='blue',marker='.')\n",
        "    x_old,y_old = x,y\n",
        "    grad = grad_f((x,y))\n",
        "    x -= alpha*grad[0]\n",
        "    y -= alpha*grad[1]\n",
        "    plt.plot([x_old,x],[y_old,y],'cyan',alpha=0.5)\n",
        "\n",
        "plt.text(x,y-0.1,f'Final point=({x:.2f},{y:.2f})',horizontalalignment='right')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Exercises\n",
        "\n",
        "1. Try increasing the step size $\\alpha$. How large can you make $\\alpha$ before gradient descent becomes unstable and does not converge? Can you estimate the Lipschitz constant $L$ of the gradient $\\nabla f$ this way? (recall that $\\alpha \\leq \\frac{1}{L}$ was our condition for convergence of gradient descent).\n",
        "2. Try using preconditioned gradient descent. For the preconditioner, use the diagonal of the Hessian matrix.\n",
        "3. Try minimizing a nonconvex function with several local minima and saddle points. For example, try the function\n",
        "$$f(x,y) = x^4 - 2x^2 + y^2.$$"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
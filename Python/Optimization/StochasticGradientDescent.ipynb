{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Stochastic gradient descent\n",
        "\n",
        "This notebook compares explores stochastic gradient descent for synthetic and real data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quadratic functions\n",
        "\n",
        "We first start with the minimization of a quadratic function with SGD. Below we define a quadratic function as the sum of 16 quadratic functions with different linear terms. SGD selects one of these pieces to perform gradient descent on at each iteration. We also define an SGD function to perform SGD for $T$ iterations with time step $\\alpha$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f(x,y):\n",
        "    return (x**2 + y**2)/2\n",
        "\n",
        "def grad(x,y):\n",
        "    p = np.random.randint(16)\n",
        "    theta = 2*np.pi*p/16\n",
        "    return x + np.cos(theta), y + np.sin(theta)\n",
        "\n",
        "def sgd(x0,y0,alpha,T):\n",
        "\n",
        "    x,y = [x0],[y0]\n",
        "    for i in range(T):\n",
        "        g = grad(x[-1],y[-1])\n",
        "        x += [x[-1]-alpha*g[0]]\n",
        "        y += [y[-1]-alpha*g[1]]\n",
        "\n",
        "    return np.array(x),np.array(y)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now test our SGD algorithm and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Plot contours\n",
        "plt.figure()\n",
        "x = np.arange(-1, 1, 0.01)\n",
        "y = np.arange(-1, 1, 0.01)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = f(X,Y)\n",
        "plt.contour(X,Y,Z, np.arange(0,1.1,0.1)**2/2.1, colors='black',linestyles='dashed')\n",
        "\n",
        "#Run SGD and plot path\n",
        "alpha = 0.1\n",
        "T = int(10/alpha)\n",
        "x,y = sgd(np.sqrt(0.5),np.sqrt(0.5),alpha,T)\n",
        "plt.plot(x,y,'C0-')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try different values for the time step $\\alpha$. How does the path taken by SGD change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST Data\n",
        "\n",
        "We now turn to experiments with real data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install -q graphlearning"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load MNIST and display some images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "\n",
        "#Load MNIST data\n",
        "x,y = gl.datasets.load('MNIST',metric='raw')\n",
        "\n",
        "#Display images\n",
        "gl.utils.image_grid(x,n_rows=16,n_cols=16)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a simple 3 layer fully connected network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 3 layer neural network with a ReLU activation function and log_softmax\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784,48)\n",
        "        self.fc2 = nn.Linear(48,24)\n",
        "        self.fc3 = nn.Linear(24,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now train the neural network with full-batch gradient descent and stochastic gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "#Training and testing data, converted to torch\n",
        "train_size = 60000\n",
        "batch_size = 64\n",
        "data = torch.from_numpy(x[:train_size,:]).float()\n",
        "target = torch.from_numpy(y[:train_size]).long()\n",
        "data_test = torch.from_numpy(x[train_size:,:]).float()\n",
        "target_test = torch.from_numpy(y[train_size:]).long()\n",
        "\n",
        "#Setup model\n",
        "model = Net()     # model that will be trained with full-batch gradient descent\n",
        "model_SGD = Net() # model that will be trained with SGD\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
        "optimizer_SGD = optim.Adadelta(model_SGD.parameters(), lr=1)\n",
        "\n",
        "#Record loss\n",
        "loss_iter = []\n",
        "loss_SGD_iter = []\n",
        "\n",
        "#Training epochs\n",
        "num_epochs = 20\n",
        "for i in range(num_epochs):\n",
        "\n",
        "    #Accuracy\n",
        "    model.eval()\n",
        "    model_SGD.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = F.nll_loss(model(data), target)\n",
        "        loss_iter.append(loss.item())\n",
        "        loss_SGD = F.nll_loss(model_SGD(data), target)\n",
        "        loss_SGD_iter.append(loss_SGD.item())\n",
        "\n",
        "        test_pred = torch.argmax(model(data_test),axis=1)\n",
        "        accuracy = torch.mean((test_pred == target_test).float())\n",
        "        test_pred = torch.argmax(model_SGD(data_test),axis=1)\n",
        "        accuracy_SGD = torch.mean((test_pred == target_test).float())\n",
        "        print('Epoch:%d, Loss:%f, Accuracy:%.2f, Loss_SGD:%f, Accuracy SGD=%.2f'%(i,loss.item(),accuracy*100,loss_SGD.item(),accuracy_SGD*100))\n",
        "\n",
        "\n",
        "    #Full batch training\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss = F.nll_loss(model(data), target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Full Batch: --- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    #SGD training\n",
        "    start_time = time.time()\n",
        "    model_SGD.train()\n",
        "    #Loop over minibatches\n",
        "    for j in range(0,train_size,batch_size):\n",
        "        data_minibatch = data[j:j+batch_size,:]\n",
        "        target_minibatch = target[j:j+batch_size]\n",
        "        optimizer_SGD.zero_grad()\n",
        "        loss_SGD = F.nll_loss(model_SGD(data_minibatch), target_minibatch)\n",
        "        loss_SGD.backward()\n",
        "        optimizer_SGD.step()\n",
        "    print(\"SGD Batch: --- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(loss_iter,label='Full-batch')\n",
        "plt.plot(loss_SGD_iter,label='Minibatch SGD')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Training Epochs')\n",
        "plt.legend()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see SGD converges far more quickly than full batch gradient descent, giving very good accuracy after only one epoch, while full batch gradient descent does not give good results even after 20 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Try other data sets, like FashionMNIST.\n",
        "2. Try playing around with the batch size."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
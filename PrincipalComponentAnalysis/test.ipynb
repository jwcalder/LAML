{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Principal Component Analyis (PCA)\n",
        "Let's play with PCA on some synthetic data first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Clean data along a line\n",
        "x = np.arange(0,1,0.01)\n",
        "y = x/2\n",
        "X = np.vstack((x,y)).T\n",
        "n = len(x)\n",
        "\n",
        "#Add some noise\n",
        "X = X + 0.05*np.random.randn(n,2)\n",
        "\n",
        "#Plot data\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1])\n",
        "plt.gca().set_aspect('equal', adjustable='box')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The synthetic data is random data with a linear trend along the line $x=y$. Let's perform PCA to find the principal components of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pca2D(X):\n",
        "    \"\"\"\n",
        "    PCA function for 2D data\n",
        "\n",
        "    Args:\n",
        "        X: nx2 array of data\n",
        "\n",
        "    Returns:\n",
        "        Vals = amounts of variation in each principal direction, largest first\n",
        "        Mean = centroid of the data\n",
        "        P = principal components of variation, as columns of matrix P\n",
        "    \"\"\"\n",
        "\n",
        "    #Compute Covariance Matrix\n",
        "    Mean = np.mean(X,axis=0)\n",
        "    cov_matrix = (X-Mean).T@(X-Mean)/X.shape[0]\n",
        "\n",
        "    #Compute eigenvalues Vals and eigenvectors P of covariance matrix\n",
        "    Vals, P = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "    #Reverse order so largest is first\n",
        "    Vals = Vals[[1,0]]\n",
        "    P = P[:,[1,0]]\n",
        "\n",
        "    return Vals,Mean,P\n",
        "\n",
        "Vals,Mean,P = pca2D(X)\n",
        "\n",
        "print('The principal components are the columns of the matrix')\n",
        "print(P)\n",
        "print('The amount of variation captured by each component is given by the eigenvalues')\n",
        "print(Vals)\n",
        "print(Vals[0]/(Vals[0] + Vals[1]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the principal components, with length given by the amount of variation. First we'll write a function for plotting, that we will call often later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Creates a visualization of PCA\n",
        "def pca_plot(X,Vals,Mean,P,padding=0.25):\n",
        "    \"\"\"\n",
        "    Creates a plot of 2D PCA, showing the principal component vectors\n",
        "    properly scaled\n",
        "\n",
        "    Args:\n",
        "        X: Data points as mxn array\n",
        "        Vals: Eigenvalues of Covariance Matrix\n",
        "        Mean: Mean of data\n",
        "        P: Matrix with Principal Components as columns\n",
        "        padding: Fraction of padding to add to sides of plot\n",
        "\n",
        "    Returns:\n",
        "        No return, just creates plot\n",
        "    \"\"\"\n",
        "\n",
        "    #Simpler variable names\n",
        "    x = X[:,0]; y = X[:,1]\n",
        "\n",
        "    #Length of arrows: 2 standard deviations contains 95% of data\n",
        "    s1 = 2*np.sqrt(Vals[0])\n",
        "    s2 = 2*np.sqrt(Vals[1])\n",
        "\n",
        "    #Create large figure and scatter points\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.scatter(x,y)\n",
        "\n",
        "    #Plot arrows\n",
        "    plt.arrow(Mean[0],Mean[1],s1*P[0,0],s1*P[1,0], head_width=0.025, color='red')\n",
        "    plt.arrow(Mean[0],Mean[1],s2*P[0,1],s2*P[1,1], head_width=0.025, color='red')\n",
        "\n",
        "    #Change xlim and ylim to add some extra padding to figure\n",
        "    plt.xlim([x.min()-padding*(x.max()-x.min()),x.max() + padding*(x.max()-x.min())])\n",
        "    plt.ylim([y.min()-padding*(y.max()-y.min()),y.max() + padding*(y.max()-y.min())])\n",
        "\n",
        "    #Set axes to be equal units\n",
        "    plt.gca().set_aspect('equal', adjustable='box')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's plot the reuslts of PCA using our function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pca_plot(X,Vals,Mean,P)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eigendigits\n",
        "\n",
        "Let's run PCA on the MNIST dataset to see an application to real data. We will use the [Graph Learning](https://github.com/jwcalder/GraphLearning) package often in the course, it can be installed in Colab with pip, as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install -q graphlearning"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "\n",
        "data, labels = gl.datasets.load('mnist')\n",
        "print(data.shape)\n",
        "gl.utils.image_grid(data, n_rows=10, n_cols=15, title='Some MNIST Images', fontsize=26)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's find the top principal components of the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy import sparse\n",
        "import numpy as np\n",
        "\n",
        "ind = labels <=1\n",
        "#Centered covariance matrix\n",
        "X = data[ind,:] - np.mean(data[ind,:],axis=0)\n",
        "S = X.T@X\n",
        "\n",
        "#Use eigsh to get subset of eigenvectors\n",
        "#('LM'=largest magnitude, k=200 eigenvectors)\n",
        "vals, vecs = sparse.linalg.eigsh(S, k=100, which='LM')\n",
        "vals, P = vals[::-1], vecs[:,::-1] #Returns in opposite order\n",
        "\n",
        "#Display the top principal component images\n",
        "gl.utils.image_grid(P.T, n_rows=10, n_cols=10, title='Top Principal Components', fontsize=26, normalize=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's hard to see anything here. Let's run PCA on each class instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_comps = 10  #Number of PCA components\n",
        "m = data.shape[1] #Number of pixels per image\n",
        "k = len(np.unique(labels)) #Number of classes\n",
        "\n",
        "#Arrays to store all principal components, and means for each class\n",
        "Means = np.zeros((k,m))\n",
        "P = np.zeros((k,m,num_comps))\n",
        "\n",
        "#Loop over all digits\n",
        "print(\"Computing PCA with %d components on each class...\"%num_comps)\n",
        "for i in range(k):\n",
        "    print('Class %d'%i)\n",
        "\n",
        "    #Extract images from class i\n",
        "    X = data[labels==i,:]\n",
        "\n",
        "    #Mean image and centered data\n",
        "    X_mean = np.mean(X,axis=0)\n",
        "    Y = X - X_mean\n",
        "\n",
        "    #Main eigenvector/eigenvalue computation\n",
        "    E, Pi = sparse.linalg.eigsh(Y.T@Y,k=num_comps,which='LM') #Principal components\n",
        "\n",
        "    #Store PCA data into arrays\n",
        "    P[i,:,:] = Pi[:,::-1]\n",
        "    Means[i,:] = X_mean"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gl.utils.image_grid(Means,n_rows=1,n_cols=k,title='Mean images of each class', fontsize=20)\n",
        "gl.utils.image_grid(np.swapaxes(P[:,:,:min(10,num_comps)],1,2),title='Principal Components',normalize=True, fontsize=20)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eigenfaces\n",
        "\n",
        "Let's do the same thing on a facial recognition dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import graphlearning as gl\n",
        "from sklearn import datasets\n",
        "\n",
        "ds = datasets.fetch_olivetti_faces()\n",
        "data = ds['data']\n",
        "labels = ds['target']\n",
        "\n",
        "gl.utils.image_grid(data, n_rows=10, n_cols=15, title='Some faces', fontsize=26)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "print(np.unique(labels))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run PCA on the face images, often called eigenfaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Centered covariance matrix\n",
        "mean_face = np.mean(data,axis=0)\n",
        "X = data - mean_face\n",
        "M = X.T@X\n",
        "\n",
        "#Use eigsh to get subset of eigenvectors\n",
        "#('LM'=largest magnitude, k=200 eigenvectors)\n",
        "vals, vecs = sparse.linalg.eigsh(M, k=100, which='LM')\n",
        "vals, P = vals[::-1], vecs[:,::-1] #Returns in opposite order\n",
        "\n",
        "#Display the top principal component images\n",
        "m = int(np.sqrt(len(mean_face)))\n",
        "plt.imshow(np.reshape(mean_face,(m,m)),cmap='gray')\n",
        "plt.title('Mean Face')\n",
        "gl.utils.image_grid(P.T, n_rows=10, n_cols=10, title='Top Principal Components (Eigenfaces)', fontsize=26, normalize=True, transpose=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Lack of robustness to outliers\n",
        "PCA can be very sensitive to outliers. Even a single outlying point can have a dramatic effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Clean data along a line\n",
        "x = np.arange(0,1,0.01)\n",
        "y = x/2\n",
        "X = np.vstack((x,y)).T\n",
        "n = len(x)\n",
        "\n",
        "#Add some noise\n",
        "X = X + 0.05*np.random.randn(n,2)\n",
        "\n",
        "#Add an extra point at (-3,1) to the dataset\n",
        "Y = np.vstack((X,[3,-1]))\n",
        "\n",
        "#Recompute PCA and plot\n",
        "Vals,Mean,P = pca2D(Y)\n",
        "pca_plot(Y,Vals,Mean,P)\n",
        "\n",
        "#Plot without the extra point, to zoom in\n",
        "pca_plot(Y[:-1,:],Vals,Mean,P)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Exercises\n",
        "\n",
        "1. Try omitting the centering step in PCA, how do things change?\n",
        "2. Add some outlying images to the face or image dataset. For example, take just the zero class in MNIST and add 1 (or a handful) of random face images. How do the principal components change?\n",
        "3. Take a couple MNIST digits, say 0 and 1, and perform PCA to find the top 2 principal components. Then scatter plot the projection $Y=XP\\in \\mathbb{R}^{m\\times 2}$ as a point cloud in the plane, with each point colored by its class label (`plt.scatter(Y[:,0],Y[:,1],c=labels)`)  \n",
        "4. Try running a clustering algorithm, like k-means from sklearn, to cluster the 2D PCA data $Y$ from part 2.\n",
        "5. Map the 2D PCA data back to the original 784 dimensional space with $P^T$, and plot the images. What do they look like? This is related to image compression, which we'll talk about next as an application of PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[1,2],[3,4]])\n",
        "Y = np.array([1,2])\n",
        "print(X)\n",
        "print(Y)\n",
        "print(Y[:,None])\n",
        "print(Y[None,:])"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}